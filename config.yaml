Model:
  CrossAttention:
    heads: 8
    dim_head: 64
  Mamba: #add dt_rank and increase d_state
    d_state: 32
    d_conv: 4
    expands: 2
    dt_rank: 16
  Unet:
    model_channels: 128
    dropout: 0.1
    time_dim: 128
  Diffuser:
    clip_model_name: "openai/clip-vit-base-patch32"
    vae_model_name: "stabilityai/sd-vae-ft-mse"
    train_timesteps: 1000

Optimizer:  #seems adamW config still kinda wrong,
  init_lr: 5e-7  #1e-6
  warmup_ratio: 0.05
  grad_acc: 4 #gradient accumulation steps, set to 1 if not using grad_acc
  Adamw:  #add beta_1 and beta_2
    base_lr: 2.5e-5  #might change to 1e-5
    weight_decay: 0.01
  Autocast: true
  ParamGroups:  #change the lr_scale and weight_decay to match the model
    mamba_block:
      lr_scale: 1.5
      weight_decay: 0.01
    CrossAttention:
      lr_scale: 1.3
      weight_decay: 0.01
    time_embed:
      lr_scale: 2.0
      weight_decay: 0.005
    scale_shift:
      lr_scale: 2.0
      weight_decay: 0.005
    input_proj:
      lr_scale: 1.0
      weight_decay: 0.005
    context_proj:
      lr_scale: 1.0
      weight_decay: 0.005

Scheduler:
  sequence: #total_iters ini enaknya sebagai epoch atau step ya? enakan epoch sih, mungkin disini epoch, tapi di create_scheduler jadi step
    - name: LinearLR
      end_factor: 1.0
      total_iters: 2500 #25000 #in steps, not epochs, maybe calculated based on warmup_ratio
    - name: CosineAnnealingWarmRestartsWithDecay   #match exact class name
      T_0: 5000 #50000  #number of step before first restart, train_subset size * num_epochs / (batch_size * grad_acc), set equal to 20 epoch, maybe change to 30 epoch initial
      freq_mult: 1.3  #frequency multiplier for subsequent restarts
      eta_min: 1e-6  #minimum learning rate
      total_iters: 50000 #500000   # Used for milestone only

#Kalo pake gradient accumulation, be careful with total_iters setting
Train:
  use_v_parameterization: true
  batch_size: 16
  num_epochs: 200
  num_workers: 2
  Dataset:
    img_size: 256
    train_dataset: "/home/arifadh/Desktop/Skripsi-Magang-Proyek/coco2017/train2017"
    val_dataset: "/home/arifadh/Desktop/Skripsi-Magang-Proyek/coco2017/val2017"
    train_annotations: "/home/arifadh/Desktop/Skripsi-Magang-Proyek/coco2017/annotations/captions_train2017.json"
    val_annotations: "/home/arifadh/Desktop/Skripsi-Magang-Proyek/coco2017/annotations/captions_val2017.json"
    train_subset: 1000 #10000
    val_subset: 100 #500
  Checkpoint:
    enabled: true #false  #true
    checkpoint_dir: "cp_advancedTrain"  # or "checkpoints_cosineAnneal" or "checkpoints_cosineDecay"
    checkpoint_path: null #"cp_advancedTrain/cp_epoch3.pt"  # null  # or "checkpoints/last.pt"
    best_checkpoint_path: "cp_advancedTrain/best_checkpoint_test.pt"  # "checkpoints/best.pt"
  EarlyStopping:
    patience: 10   #in epochs
    min_progress: 0.001

Validate:
  denoise_step: 50
  guidance_scale: 7.5
Deploy:
  deploy_as: "onnx"  # or "engine"
  precision: "fp16"  # or "fp32"
