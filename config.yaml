Model:
  CrossAttention:
    heads: 8
    dim_head: 64
  Mamba: #add dt_rank and increase d_state
    d_state: 32
    d_conv: 4
    expands: 2
    dt_rank: 16
  Unet:
    model_channels: 128
    dropout: 0.1
    time_dim: 128
  Diffuser:
    clip_model_name: "openai/clip-vit-base-patch32"
    vae_model_name: "stabilityai/sd-vae-ft-mse"
    train_timesteps: 1000

Optimizer:  #seems adamW config still kinda wrong,
  init_lr: 5e-7  #1e-6
  grad_acc: 4 #gradient accumulation steps, set to 1 if not using grad_acc
  amp_dtype: "fp16"
  Adamw:  #add beta_1 and beta_2
    base_lr: 1.2e-5  #might change to 1.5e-5
    weight_decay: 0.008
    betas: [0.9, 0.95]
  ParamGroups:
    mamba_block:
      lr_scale: 2.0            # Increase core learning
      weight_decay: 0.005
    CrossAttention:
      lr_scale: 1.8            # Increase cross-modal learning
      weight_decay: 0.005
    time_embed:
      lr_scale: 2.5            # Keep high
      weight_decay: 0.004
    scale_shift:
      lr_scale: 2.5            # Keep high
      weight_decay: 0.004
    input_proj:
      lr_scale: 0.6            # Reduce frozen interface
      weight_decay: 0.01
    context_proj:
      lr_scale: 0.6            # Reduce frozen interface
      weight_decay: 0.01

Scheduler:
  sequence: #total_iters ini enaknya sebagai epoch atau step ya? enakan epoch sih, mungkin disini epoch, tapi di create_scheduler jadi step
    - name: LinearLR
      end_factor: 1.0
      total_iters: 16 #25000 #in steps, not epochs, maybe calculated based on warmup_ratio, ini setara 10 epoch
    - name: CosineAnnealingWarmRestartsWithDecay   #match exact class name
      T_0: 40 #50000  #number of step before first restart, train_subset size * num_epochs / (batch_size * grad_acc), set equal to 20 epoch, maybe change to 30 epoch initial
      freq_mult: 1.0  #frequency multiplier for subsequent restarts
      eta_min: 1e-7  #minimum learning rate
      total_iters: 200 #500000   # Used for milestone only

#Kalo pake gradient accumulation, be careful with total_iters setting
Train:
  use_v_parameterization: true
  batch_size: 16
  num_epochs: 216
  num_workers: 2
  Dataset:
    img_size: 256
    train_dataset: "/home/arifadh/Desktop/Skripsi-Magang-Proyek/coco2017/train2017"
    val_dataset: "/home/arifadh/Desktop/Skripsi-Magang-Proyek/coco2017/val2017"
    train_annotations: "/home/arifadh/Desktop/Skripsi-Magang-Proyek/coco2017/annotations/captions_train2017.json"
    val_annotations: "/home/arifadh/Desktop/Skripsi-Magang-Proyek/coco2017/annotations/captions_val2017.json"
    train_subset: 10000 #10000
    val_subset: 500 #500
  Checkpoint:
    enabled: true #false  #true
    checkpoint_dir: "cp_advancedTrain"  # or "checkpoints_cosineAnneal" or "checkpoints_cosineDecay"
    checkpoint_path: "cp_advancedTrain/cp_epoch85.pt"  # null  # or "checkpoints/last.pt"
    best_checkpoint_path: "best_checkpoint_test.pt"  # "checkpoints/best.pt"
  EarlyStopping:
    patience: 25   #in epochs
    min_delta: 0.0001

Validate:
  denoise_step: 50
  guidance_scale: 7.5
Deploy:
  deploy_as: "onnx"  # or "engine"
  precision: "fp16"  # or "fp32"
